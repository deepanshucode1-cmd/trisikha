name: Weekly Database Backup

on:
  schedule:
    # Every Sunday at 2:00 AM IST (Saturday 8:30 PM UTC)
    - cron: '30 20 * * 0'
  workflow_dispatch: # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client-16

       - name: Force IPv4 (GitHub Actions has limited IPv6 support)
        run: |
          sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1    

      - name: Create database backup
        env:
          SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="backup_${TIMESTAMP}.sql"

          echo "Starting backup at $(date)"

          # Create backup using pg_dump
          pg_dump "$SUPABASE_DB_URL" \
            --no-owner \
            --no-acl \
            --disable-triggers \
            --format=plain \
            > "$BACKUP_FILE"

          # Check if backup was created
          if [ ! -f "$BACKUP_FILE" ]; then
            echo "ERROR: Backup file was not created"
            exit 1
          fi

          # Compress the backup
          gzip "$BACKUP_FILE"

          # Generate checksum for integrity verification
          sha256sum "${BACKUP_FILE}.gz" > "${BACKUP_FILE}.gz.sha256"

          # Export for subsequent steps
          echo "BACKUP_FILE=${BACKUP_FILE}.gz" >> $GITHUB_ENV
          echo "CHECKSUM_FILE=${BACKUP_FILE}.gz.sha256" >> $GITHUB_ENV
          echo "TIMESTAMP=${TIMESTAMP}" >> $GITHUB_ENV

          echo "Backup created: ${BACKUP_FILE}.gz"

      - name: Verify backup integrity
        run: |
          # Check file exists
          if [ ! -f "$BACKUP_FILE" ]; then
            echo "ERROR: Backup file not found"
            exit 1
          fi

          # Check file size (should be > 5KB for non-empty DB)
          FILE_SIZE=$(stat -c%s "$BACKUP_FILE")
          echo "Backup file size: $FILE_SIZE bytes"

          if [ "$FILE_SIZE" -lt 5120 ]; then
            echo "ERROR: Backup file too small ($FILE_SIZE bytes) - may be empty or corrupted"
            exit 1
          fi

          # Verify critical tables exist in backup
          echo "Checking for critical tables..."

          TABLES_TO_CHECK=("products" "orders" "order_items" "user_role")
          for table in "${TABLES_TO_CHECK[@]}"; do
            if zcat "$BACKUP_FILE" | grep -q "CREATE TABLE.*${table}\|COPY.*${table}"; then
              echo "  ✓ Found table: $table"
            else
              echo "  ⚠ Table '$table' not found (may be empty, continuing...)"
            fi
          done

          # Verify checksum file
          sha256sum -c "$CHECKSUM_FILE"

          echo "Backup integrity verification passed"

      - name: Upload to Cloudflare R2
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          # Configure AWS CLI for Cloudflare R2 (S3-compatible)
          aws configure set aws_access_key_id "$R2_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$R2_SECRET_ACCESS_KEY"
          aws configure set default.region auto

          R2_ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          # Upload backup file
          aws s3 cp "$BACKUP_FILE" "s3://${R2_BUCKET}/backups/${BACKUP_FILE}" \
            --endpoint-url "$R2_ENDPOINT"

          # Upload checksum file
          aws s3 cp "$CHECKSUM_FILE" "s3://${R2_BUCKET}/backups/${CHECKSUM_FILE}" \
            --endpoint-url "$R2_ENDPOINT"

          echo "Uploaded to Cloudflare R2: ${R2_BUCKET}/backups/${BACKUP_FILE}"

          # List recent backups
          echo ""
          echo "Recent backups in R2:"
          aws s3 ls "s3://${R2_BUCKET}/backups/" --endpoint-url "$R2_ENDPOINT" | tail -10

      - name: Cleanup old backups (keep last 8 weeks)
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          R2_ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '56 days ago' +%Y-%m-%d)
          echo "Cleaning up backups older than $CUTOFF_DATE"

          aws s3 ls "s3://${R2_BUCKET}/backups/" --endpoint-url "$R2_ENDPOINT" | while read -r line; do
            FILE_DATE=$(echo "$line" | awk '{print $1}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')

            if [[ "$FILE_DATE" < "$CUTOFF_DATE" ]] && [[ -n "$FILE_NAME" ]]; then
              echo "Deleting old backup: $FILE_NAME"
              aws s3 rm "s3://${R2_BUCKET}/backups/${FILE_NAME}" --endpoint-url "$R2_ENDPOINT"
            fi
          done

      - name: Summary
        env:
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          echo "## Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Timestamp | $TIMESTAMP |" >> $GITHUB_STEP_SUMMARY
          echo "| Backup File | $BACKUP_FILE |" >> $GITHUB_STEP_SUMMARY
          echo "| File Size | $(stat -c%s "$BACKUP_FILE") bytes |" >> $GITHUB_STEP_SUMMARY
          echo "| Checksum | $(cat "$CHECKSUM_FILE" | cut -d' ' -f1) |" >> $GITHUB_STEP_SUMMARY
          echo "| R2 Location | ${R2_BUCKET}/backups/$BACKUP_FILE |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Backup completed successfully at $(date)" >> $GITHUB_STEP_SUMMARY
