name: Daily Database Backup

on:
  schedule:
    # Every day at 2:00 AM IST (8:30 PM UTC previous day)
    - cron: '30 20 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Install PostgreSQL 17 client
        run: |
          # Add PostgreSQL apt repository for version 17
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          curl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17
          # Verify correct version is installed
          /usr/lib/postgresql/17/bin/pg_dump --version

      - name: Force IPv4 (GitHub Actions has limited IPv6 support)
        run: |
          sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1

      - name: Create database backup
        env:
          SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_FILE="backup_${TIMESTAMP}.dump"

          echo "Starting backup at $(date)"

          # Create backup using pg_dump (explicit path to v17)
          # --format=custom: Compressed, allows selective restore
          # --blobs: Include large objects
          # --verbose: Detailed logging
          # --schema=public: User data only (excludes Supabase internal schemas)
          # --no-owner/--no-acl: Supabase manages roles separately
          /usr/lib/postgresql/17/bin/pg_dump "$SUPABASE_DB_URL" \
            --format=custom \
            --blobs \
            --verbose \
            --schema=public \
            --no-owner \
            --no-acl \
            --file="$BACKUP_FILE"

          # Check if backup was created
          if [ ! -f "$BACKUP_FILE" ]; then
            echo "ERROR: Backup file was not created"
            exit 1
          fi

          # Generate checksum for integrity verification
          sha256sum "$BACKUP_FILE" > "${BACKUP_FILE}.sha256"

          # Export for subsequent steps
          echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV
          echo "CHECKSUM_FILE=${BACKUP_FILE}.sha256" >> $GITHUB_ENV
          echo "TIMESTAMP=${TIMESTAMP}" >> $GITHUB_ENV

          echo "Backup created: ${BACKUP_FILE}"

      - name: Verify backup integrity
        run: |
          # Check file exists
          if [ ! -f "$BACKUP_FILE" ]; then
            echo "ERROR: Backup file not found"
            exit 1
          fi

          # Check file size (should be > 1KB for non-empty DB)
          FILE_SIZE=$(stat -c%s "$BACKUP_FILE")
          echo "Backup file size: $FILE_SIZE bytes"

          if [ "$FILE_SIZE" -lt 1024 ]; then
            echo "ERROR: Backup file too small ($FILE_SIZE bytes) - may be empty or corrupted"
            exit 1
          fi

          # List contents of backup to verify structure
          echo "Backup contents:"
          /usr/lib/postgresql/17/bin/pg_restore --list "$BACKUP_FILE" | head -50

          # Verify critical tables exist in backup
          echo ""
          echo "Checking for critical tables..."
          TABLES_TO_CHECK=("products" "orders" "order_items" "user_role")
          for table in "${TABLES_TO_CHECK[@]}"; do
            if /usr/lib/postgresql/17/bin/pg_restore --list "$BACKUP_FILE" | grep -q "TABLE.*${table}"; then
              echo "  âœ“ Found table: $table"
            else
              echo "  âš  Table '$table' not found (may be empty, continuing...)"
            fi
          done

          # Verify checksum file
          sha256sum -c "$CHECKSUM_FILE"

          echo "Backup integrity verification passed"

      - name: Upload to Cloudflare R2
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          # Configure AWS CLI for Cloudflare R2 (S3-compatible)
          aws configure set aws_access_key_id "$R2_ACCESS_KEY_ID"
          aws configure set aws_secret_access_key "$R2_SECRET_ACCESS_KEY"
          aws configure set default.region auto

          R2_ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          # Upload backup file
          aws s3 cp "$BACKUP_FILE" "s3://${R2_BUCKET}/backups/${BACKUP_FILE}" \
            --endpoint-url "$R2_ENDPOINT"

          # Upload checksum file
          aws s3 cp "$CHECKSUM_FILE" "s3://${R2_BUCKET}/backups/${CHECKSUM_FILE}" \
            --endpoint-url "$R2_ENDPOINT"

          echo "Uploaded to Cloudflare R2: ${R2_BUCKET}/backups/${BACKUP_FILE}"

          # List recent backups
          echo ""
          echo "Recent backups in R2:"
          aws s3 ls "s3://${R2_BUCKET}/backups/" --endpoint-url "$R2_ENDPOINT" | tail -10

      - name: Cleanup old backups (keep last 30 days)
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          R2_ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y-%m-%d)
          echo "Cleaning up backups older than $CUTOFF_DATE"

          aws s3 ls "s3://${R2_BUCKET}/backups/" --endpoint-url "$R2_ENDPOINT" | while read -r line; do
            FILE_DATE=$(echo "$line" | awk '{print $1}')
            FILE_NAME=$(echo "$line" | awk '{print $4}')

            if [[ "$FILE_DATE" < "$CUTOFF_DATE" ]] && [[ -n "$FILE_NAME" ]]; then
              echo "Deleting old backup: $FILE_NAME"
              aws s3 rm "s3://${R2_BUCKET}/backups/${FILE_NAME}" --endpoint-url "$R2_ENDPOINT"
            fi
          done

      - name: Summary
        env:
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          echo "## Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Timestamp | $TIMESTAMP |" >> $GITHUB_STEP_SUMMARY
          echo "| Backup File | $BACKUP_FILE |" >> $GITHUB_STEP_SUMMARY
          echo "| File Size | $(stat -c%s "$BACKUP_FILE") bytes |" >> $GITHUB_STEP_SUMMARY
          echo "| Checksum | $(cat "$CHECKSUM_FILE" | cut -d' ' -f1) |" >> $GITHUB_STEP_SUMMARY
          echo "| R2 Location | ${R2_BUCKET}/backups/$BACKUP_FILE |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Restore Instructions" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
          echo "# Download from R2" >> $GITHUB_STEP_SUMMARY
          echo "aws s3 cp s3://${R2_BUCKET}/backups/$BACKUP_FILE ./$BACKUP_FILE --endpoint-url \"\\\$R2_ENDPOINT\"" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "# Verify checksum" >> $GITHUB_STEP_SUMMARY
          echo "sha256sum -c ${BACKUP_FILE}.sha256" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "# Restore to database" >> $GITHUB_STEP_SUMMARY
          echo "pg_restore --verbose --no-owner --no-acl --clean --if-exists -d \"\\\$DATABASE_URL\" $BACKUP_FILE" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Backup completed successfully at $(date)" >> $GITHUB_STEP_SUMMARY

  # Failure notification job - runs if backup job fails
  notify-failure:
    runs-on: ubuntu-latest
    needs: backup
    if: failure()

    steps:
      - name: Create GitHub Issue for backup failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸš¨ CRITICAL: Database Backup Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `## Database Backup Failure Alert

            **Severity:** CRITICAL (DPDP Availability Breach Risk)
            **Time:** ${new Date().toISOString()}
            **Workflow Run:** [View Details](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ### Impact
            - Daily backup did not complete successfully
            - Data recovery capability may be compromised
            - This is a reportable incident under DPDP Act (Loss of Availability)

            ### Required Actions
            1. **Immediate:** Check workflow logs for failure reason
            2. **Within 1 hour:** Manually trigger backup or fix the issue
            3. **Document:** Log this incident in the security dashboard

            ### Possible Causes
            - Database connection issues
            - Cloudflare R2 storage issues
            - GitHub Actions runner issues
            - Insufficient storage space

            ### Labels
            - \`security\`
            - \`critical\`
            - \`backup-failure\`

            ---
            *This issue was automatically created by the backup monitoring system.*
            `;

            // Check if there's already an open issue for today
            const today = new Date().toISOString().split('T')[0];
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'backup-failure',
            });

            const todayIssue = existingIssues.data.find(issue =>
              issue.title.includes(today)
            );

            if (!todayIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['security', 'critical', 'backup-failure'],
              });
              console.log('Created new issue for backup failure');
            } else {
              // Add comment to existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: todayIssue.number,
                body: `### Additional Failure at ${new Date().toISOString()}\n\nBackup failed again. [View workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`,
              });
              console.log('Added comment to existing issue');
            }

      - name: Send webhook notification (optional)
        if: ${{ secrets.BACKUP_ALERT_WEBHOOK_URL != '' }}
        env:
          WEBHOOK_URL: ${{ secrets.BACKUP_ALERT_WEBHOOK_URL }}
        run: |
          curl -X POST "$WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d '{
              "event": "backup_failure",
              "severity": "critical",
              "incident_type": "backup_failure",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
              "details": {
                "workflow_run": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}",
                "repository": "${{ github.repository }}"
              }
            }' || echo "Webhook notification failed (non-critical)"
